{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"5HExLQrE4ZxR"},"source":["<h1><font color='blue'> 8E and 8F: Finding the Probability P(Y==1|X)</font></h1>"]},{"cell_type":"markdown","metadata":{"id":"4LuKrFzC4ZxV"},"source":["<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"]},{"cell_type":"markdown","metadata":{"id":"1wES-wWN4ZxX"},"source":["<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n","\n","Check the documentation for better understanding of these attributes: \n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n","<img src='https://i.imgur.com/K11msU4.png' width=500>\n","\n","As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n","\n","Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n","\n","Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n","\n","Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n","$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n","\n","RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n","\n","For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"z830CfMk4Zxa"},"source":["## Task E"]},{"cell_type":"markdown","metadata":{"id":"MuBxHiCQ4Zxc"},"source":["> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n","\n","> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n","\n","> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"]},{"cell_type":"code","metadata":{"id":"fCgMNEvI4Zxf"},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","from sklearn.svm import SVC\n","import math\n","from functools import reduce"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANUNIqCe4Zxn"},"source":["X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n","                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tHie1zqH4Zxt"},"source":["### Pseudo code\n","\n","clf = SVC(gamma=0.001, C=100.)<br>\n","clf.fit(Xtrain, ytrain)\n","\n","<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n","   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n","    \n","fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n","\n","<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"]},{"cell_type":"code","metadata":{"id":"h43kDT3M41u5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667029474542,"user_tz":-330,"elapsed":5,"user":{"displayName":"santhosh kurnapally","userId":"15491120331264307379"}},"outputId":"18237bb1-dc7e-4a23-d906-c7c3af52d41a"},"source":["# you can write your code here\n","X_train, X_test,y_train,y_test = train_test_split(X,y,test_size = 0.20,stratify = y, random_state = 42)\n","X_train, X_cv,y_train,y_cv = train_test_split(X_train,y_train,test_size = 0.25,stratify = y_train, random_state = 42)\n","print(X_train.shape)\n","print(y_train.shape)\n","print(X_cv.shape)\n","print(y_cv.shape)\n","print(X_test.shape)\n","print(y_test.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(3000, 5)\n","(3000,)\n","(1000, 5)\n","(1000,)\n","(1000, 5)\n","(1000,)\n"]}]},{"cell_type":"code","source":["clf = SVC(gamma=0.001, C=100.0)\n","clf.fit(X_train, y_train)\n","intercept = clf.intercept_\n","alphas = clf.dual_coef_.ravel()\n","support_vectors = clf.support_vectors_\n","print(intercept)\n","print(alphas.shape)\n","print(support_vectors.shape)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7AEESY3tn0ym","executionInfo":{"status":"ok","timestamp":1667029476281,"user_tz":-330,"elapsed":616,"user":{"displayName":"santhosh kurnapally","userId":"15491120331264307379"}},"outputId":"8470c138-b422-4128-fe04-f1624b6487da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2.57656467]\n","(571,)\n","(571, 5)\n"]}]},{"cell_type":"code","source":["def radial_kernel(xi,xq):\n","  #print('-'*30)\n","  #print(np.exp(-0.001 * (np.linalg.norm(xi - xq))**2))\n","  #print('-'*30)\n","  return np.exp(-0.001 * (np.linalg.norm(xi - xq))**2)\n"],"metadata":{"id":"MDjer5_38uke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def decision_function(x,alphas,intercept,support_vectors):\n","  decision_function_output = []\n","  for row in range(len(x)):\n","    decision_function_each = 0\n","    for sp in range(len(y_support_vectors)):\n","      y_support_vector = y_support_vectors[sp]\n","      alpha = alphas[sp]\n","      rbf = radial_kernel(x[row],support_vectors[sp])\n","      #print((y_support_vector * alpha * rbf))\n","      #print(intercept[0])\n","      #print(alpha)\n","      #print(rbf)\n","      #print(y_support_vector[0])\n","      decision_function_each += (alpha * rbf)\n","      #print(decision_function_each)\n","    decision_function_each+=intercept[0]\n","    #print(decision_function_each)\n","    decision_function_output.append(decision_function_each)\n","    #print('*'*50)\n","    #print(decision_function_output)\n","  return decision_function_output\n"],"metadata":{"id":"Jj-mvr_WoHnW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fcv = decision_function(X_cv,alphas,intercept,support_vectors)"],"metadata":{"id":"oHuqzrdM-uzZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_decisionvalues = clf.decision_function(X_cv)\n"],"metadata":{"id":"QqDOTCySBUvC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.DataFrame(data = np.vstack((np.array(fcv),model_decisionvalues)), index = ['fcv','model_decision_values']).T\n","print(data.head(10))\n","print(data.tail(10))\n","print(\"differnce between these two fcv and model_decision_values : \", format((np.array(fcv) - model_decisionvalues).sum(),'f'))\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fdCd4xpHXR58","executionInfo":{"status":"ok","timestamp":1667029516526,"user_tz":-330,"elapsed":3,"user":{"displayName":"santhosh kurnapally","userId":"15491120331264307379"}},"outputId":"229bf8b9-aa3d-4fc2-a688-f25f7196555b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["        fcv  model_decision_values\n","0  2.157101               2.157101\n","1  0.018191               0.018191\n","2 -2.709886              -2.709886\n","3 -0.423798              -0.423798\n","4  2.815841               2.815841\n","5 -1.830484              -1.830484\n","6  1.527454               1.527454\n","7 -1.471302              -1.471302\n","8  2.357664               2.357664\n","9  2.866437               2.866437\n","          fcv  model_decision_values\n","990 -3.472022              -3.472022\n","991 -5.221700              -5.221700\n","992 -0.798153              -0.798153\n","993 -3.026031              -3.026031\n","994 -3.689129              -3.689129\n","995 -1.516216              -1.516216\n","996  0.768396               0.768396\n","997 -2.875956              -2.875956\n","998 -2.607290              -2.607290\n","999 -3.141415              -3.141415\n","differnce between these two fcv and model_decision_values :  0.000000\n"]}]},{"cell_type":"markdown","metadata":{"id":"c0bKCboN4Zxu"},"source":["<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"]},{"cell_type":"markdown","metadata":{"id":"nMn7OEN94Zxw"},"source":["Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n","<img src='https://i.imgur.com/CAMnVnh.png'>\n"]},{"cell_type":"markdown","metadata":{"id":"e0n5EFkx4Zxz"},"source":["## TASK F"]},{"cell_type":"markdown","metadata":{"id":"t0HOqVJq4Zx1"},"source":["\n","> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n","\n","> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n","\n","> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n","<img src='https://i.imgur.com/zKYE9Oc.png'>\n","if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n","\n","> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"]},{"cell_type":"markdown","metadata":{"id":"oTY7z2bd4Zx2"},"source":["__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"]},{"cell_type":"markdown","metadata":{"id":"CM3odN1Z4Zx3"},"source":["\n","If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n","\n","1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n","\n","2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n","\n","3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n","\n","4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"]},{"cell_type":"markdown","source":["# implementing step 4 : Apply SGD algorithm with ( 𝑓𝑐𝑣 ,  𝑦𝑐𝑣 ) and find the weight  𝑊  intercept  𝑏  Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)"],"metadata":{"id":"gjIqf5V9sFBV"}},{"cell_type":"code","source":["y_cv_for_calib = pd.Series(y_cv).map({1:(y_cv.sum() + 1)/(y_cv.sum()+2), 0: 1/((len(y_cv) - (y_cv.sum() + 1)/(y_cv.sum()+2))+2)}).values\n","y_cv_for_calib\n"],"metadata":{"id":"L-YFy16qjXjf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dUmNAPWe3rso"},"outputs":[],"source":["def initialize_weights_bias(dim):\n","    ''' In this function, we will initialize our weights and bias terms'''\n","\n","    # Initialize the weights to zeros array of (dim) dimensions. Here dim will be the number of features of your tfidf vectorizer output.\n","    # You can initialize the weight terms with zeros.\n","    # Initialize bias term to zero\n","    # Write your code below.\n","    w = np.zeros(dim)\n","    b = 0\n","    return w,b"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FfP2Z7gv3JpX"},"outputs":[],"source":["def custom_sigmoid(z):\n","    ''' In this function, we will return sigmoid of z'''\n","    \n","    # Compute sigmoid(z) and return its value.\n","    # Write your code below.\n","    sigmoid = 1/(1+math.exp(-z))\n","    return sigmoid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"41ush74x8ueR"},"outputs":[],"source":["def custom_loss(y_true, y_pred, alpha, w):\n","    '''In this function, we will compute total loss which is [(logloss) + (alpha * L1regularization loss)] '''\n","    \n","    # Write your code below.\n","    log_loss = -1 * (reduce(lambda x, y: x + y, [(y_true[i]*math.log10(y_pred[i]) + (1-y_true[i])*math.log10(1-y_pred[i])) for i in range(len(y_true))]))/len(y_true)\n","    l1_loss = reduce(lambda x,y : x+y,w )\n","    total_loss = log_loss + (alpha*l1_loss)\n","\n","    return total_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hREi7bQxM8h-"},"outputs":[],"source":["def gradient_dw(x, y, w, b, alpha, N):\n","    '''In this function, we will compute the gardient w.r.t. w '''\n","    # Write your code below.\n","    dw = (((custom_sigmoid(np.dot(x,w)+b) - y)*x) + ((alpha/N) * ((w+math.exp(-5))/np.linalg.norm(w+math.exp(-5)))))\n","    return dw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mIMxwuBAND0a"},"outputs":[],"source":["def gradient_db(x, y, w, b):\n","    '''In this function, we will compute the gardient w.r.t. b '''\n","    \n","    # Write your code below.\n","    db = (custom_sigmoid(np.dot(x,w)+b) - y)\n","    return db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44nod2Gu-ZWq"},"outputs":[],"source":["def custom_train(X_train, y_train,alpha, eta0,tolerance):\n","  \"\"\"\n","  In this function we will compute optimal values for weights and bias terms on\n","  the train data. \n","\n","  Here eta0 is the learning rate and alpha is the regularization term.\n","  \"\"\"\n","  train_loss=[]\n","  # Implement the code as follows:\n","\n","  # 1. Initalize the weights (call the initialize_weights(X_train[0]) function)\n","  # 2. Repeat For many epochs until condition \"e\"  fails\n","          # a) for every data point(X_train,y_train)\n","                # compute gradient w.r.to w (call the gradient_dw() function)\n","                # compute gradient w.r.to b (call the gradient_db() function)\n","                # update w, b using the above eqns\n","          # b) predict the output of x_train[for all data points in X_train] using w,b\n","          # c) compute the loss between predicted and actual values (call the loss function)\n","          # d) store all the train loss values in a list\n","          # e) Compare previous loss and current loss, if the difference between loss is not more than or equal to the tolerance, stop the process and return w,b\n","\n","  # 3. Return the values of weights, bias, train_loss and num_epochs \n","  w,b=initialize_weights_bias(X_train.shape[1])\n","  N=len(X_train)\n","  num_epochs=0\n","  cond=True\n","  while(cond):\n","    for j in range(len(X_train)):\n","        x=X_train[j]\n","        y=y_train[j]\n","        dw=gradient_dw(x,y,w,b,alpha,N)\n","        db=gradient_db(x,y,w,b)\n","        w=w-eta0*dw\n","        b=b-eta0*db\n","    y_pred_train= np.array([custom_sigmoid(np.dot(w,X_Value)+b) for X_Value in X_train])\n","    train_loss.append(custom_loss(y_train,y_pred_train,alpha,w))\n","    if num_epochs>0:\n","      cond=((train_loss[num_epochs-1]-train_loss[num_epochs])>=tolerance)\n","    num_epochs=num_epochs+1 \n","  return w,b,train_loss,num_epochs\n"]},{"cell_type":"code","source":["w,b,train_loss,epochs = custom_train(np.array(fcv).reshape(-1,1), y_cv_for_calib, 0.0001,0.0001,0.001)\n","print(\"trained weight vector : \", w)\n","print(\"trained bias : \", b)\n","print(\"optimised train loss : \", train_loss[-1])\n","print(\"Number of Epochs : \", epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TwvdwobtoWW0","executionInfo":{"status":"ok","timestamp":1667029539346,"user_tz":-330,"elapsed":398,"user":{"displayName":"santhosh kurnapally","userId":"15491120331264307379"}},"outputId":"87158eb0-6a12-427b-9eb7-4dd454d69978"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["trained weight vector :  [0.97599598]\n","trained bias :  -0.16673563191510346\n","optimised train loss :  0.10379000686346558\n","Number of Epochs :  29\n"]}]},{"cell_type":"markdown","source":["# implementin step 5: For a given data point from  𝑋𝑡𝑒𝑠𝑡 ,  𝑃(𝑌=1|𝑋)=1/1+𝑒𝑥𝑝(−(𝑊∗𝑓𝑡𝑒𝑠𝑡+𝑏))  where  𝑓𝑡𝑒𝑠𝑡  = decision_function( 𝑋𝑡𝑒𝑠𝑡 ), W and b will be learned as metioned in the above step"],"metadata":{"id":"6Mh13UfCsI2x"}},{"cell_type":"code","source":["ftest = decision_function(X_test,alphas,intercept,support_vectors)"],"metadata":{"id":"lyRZBTYurHVD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test_pred_proba = 1/(1+np.exp(-((w[0]* np.array(ftest)) + b)))\n"],"metadata":{"id":"HxbSsAdati0P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["convert = lambda x : 1 if(x > 0.5) else 0\n","y_test_pred = np.array([convert(xi) for xi in y_test_pred_proba])\n","print('accuracy after the platts scaling is :',accuracy_score(y_test,y_test_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rhALRi6FuC10","executionInfo":{"status":"ok","timestamp":1667030142493,"user_tz":-330,"elapsed":4,"user":{"displayName":"santhosh kurnapally","userId":"15491120331264307379"}},"outputId":"c88d9956-aec9-491a-913f-2ad47c3168b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy after the platts scaling is : 0.938\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"eB68mauKuF4N"},"execution_count":null,"outputs":[]}]}
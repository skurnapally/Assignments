{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17,"output_embedded_package_id":"1B38FM_Za_3l3dcp-ap4ZuoJ6LCPCJt7g"},"id":"d38hGDTBZbmU","executionInfo":{"status":"ok","timestamp":1666084967587,"user_tz":-330,"elapsed":3889,"user":{"displayName":"santhosh kurnapally","userId":"15491120331264307379"}},"outputId":"2f4082bc-a659-4d13-eed7-ddf5efad5daf"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["%matplotlib inline\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import pandas as pd\n","import numpy as np\n","import nltk\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import confusion_matrix\n","from sklearn import metrics\n","from sklearn.metrics import roc_curve, auc\n","\n","import re\n","# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n","\n","import pickle\n","from tqdm import tqdm\n","import os\n","\n","#! pip install chart_studio\n","from chart_studio import plotly\n","import plotly.offline as offline\n","import plotly.graph_objs as go\n","offline.init_notebook_mode()\n","from collections import Counter"]},{"cell_type":"markdown","metadata":{"id":"LCw3wa0vZbmV"},"source":["## 1. Loading Data"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4h7mvupCZ0wJ","executionInfo":{"status":"ok","timestamp":1666085006953,"user_tz":-330,"elapsed":21415,"user":{"displayName":"santhosh kurnapally","userId":"15491120331264307379"}},"outputId":"bc664a2e-9bfe-4248-cc88-b38345c396eb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":704},"id":"nK3veWpbZbmW","executionInfo":{"status":"ok","timestamp":1666085032032,"user_tz":-330,"elapsed":4309,"user":{"displayName":"santhosh kurnapally","userId":"15491120331264307379"}},"outputId":"3a4b1143-39ae-4eb0-c128-2044eee82295"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["  school_state teacher_prefix project_grade_category  \\\n","0           ca            mrs          grades_prek_2   \n","1           ut             ms             grades_3_5   \n","2           ca            mrs          grades_prek_2   \n","3           ga            mrs          grades_prek_2   \n","4           wa            mrs             grades_3_5   \n","\n","   teacher_number_of_previously_posted_projects  project_is_approved  \\\n","0                                            53                    1   \n","1                                             4                    1   \n","2                                            10                    1   \n","3                                             2                    1   \n","4                                             2                    1   \n","\n","    clean_categories                 clean_subcategories  \\\n","0       math_science  appliedsciences health_lifescience   \n","1       specialneeds                        specialneeds   \n","2  literacy_language                            literacy   \n","3    appliedlearning                    earlydevelopment   \n","4  literacy_language                            literacy   \n","\n","                                               essay   price  \n","0  i fortunate enough use fairy tale stem kits cl...  725.05  \n","1  imagine 8 9 years old you third grade classroo...  213.03  \n","2  having class 24 students comes diverse learner...  329.00  \n","3  i recently read article giving students choice...  481.04  \n","4  my students crave challenge eat obstacles brea...   17.74  "],"text/html":["\n","  <div id=\"df-9f44dc2d-d7eb-4d4a-bb66-50fff3229c6b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>school_state</th>\n","      <th>teacher_prefix</th>\n","      <th>project_grade_category</th>\n","      <th>teacher_number_of_previously_posted_projects</th>\n","      <th>project_is_approved</th>\n","      <th>clean_categories</th>\n","      <th>clean_subcategories</th>\n","      <th>essay</th>\n","      <th>price</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>ca</td>\n","      <td>mrs</td>\n","      <td>grades_prek_2</td>\n","      <td>53</td>\n","      <td>1</td>\n","      <td>math_science</td>\n","      <td>appliedsciences health_lifescience</td>\n","      <td>i fortunate enough use fairy tale stem kits cl...</td>\n","      <td>725.05</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ut</td>\n","      <td>ms</td>\n","      <td>grades_3_5</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>specialneeds</td>\n","      <td>specialneeds</td>\n","      <td>imagine 8 9 years old you third grade classroo...</td>\n","      <td>213.03</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ca</td>\n","      <td>mrs</td>\n","      <td>grades_prek_2</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>literacy_language</td>\n","      <td>literacy</td>\n","      <td>having class 24 students comes diverse learner...</td>\n","      <td>329.00</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ga</td>\n","      <td>mrs</td>\n","      <td>grades_prek_2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>appliedlearning</td>\n","      <td>earlydevelopment</td>\n","      <td>i recently read article giving students choice...</td>\n","      <td>481.04</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>wa</td>\n","      <td>mrs</td>\n","      <td>grades_3_5</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>literacy_language</td>\n","      <td>literacy</td>\n","      <td>my students crave challenge eat obstacles brea...</td>\n","      <td>17.74</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9f44dc2d-d7eb-4d4a-bb66-50fff3229c6b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9f44dc2d-d7eb-4d4a-bb66-50fff3229c6b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9f44dc2d-d7eb-4d4a-bb66-50fff3229c6b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}],"source":["data  = pd.read_csv('/content/drive/MyDrive/6_Donors_choose_NB/preprocessed_data.csv')\n","# data  = pd.read_csv('/content/drive/MyDrive/6_Donors_choose_NB/preprocessed_data.csv', nrows=50000) # you can take less number of rows like this\n","data.head(5)"]},{"cell_type":"markdown","metadata":{"id":"HppE2nteZbmW"},"source":["# 2. Vectorizing Text data"]},{"cell_type":"markdown","metadata":{"id":"EhtzIklGZbmW"},"source":["## 2.1 Bag of words"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"iXQn9hfyZbmW","executionInfo":{"status":"ok","timestamp":1666085080305,"user_tz":-330,"elapsed":5,"user":{"displayName":"santhosh kurnapally","userId":"15491120331264307379"}}},"outputs":[],"source":["preprocessed_essays = data['essay'].values"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M-zCN2BIZbmW","executionInfo":{"status":"ok","timestamp":1666085104773,"user_tz":-330,"elapsed":12899,"user":{"displayName":"santhosh kurnapally","userId":"15491120331264307379"}},"outputId":"8b83fc85-e42a-4bb7-9419-96e92a06508d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of matrix after one hot encodig  (109248, 16623)\n"]}],"source":["# We are considering only the words which appeared in at least 10 documents(rows or projects).\n","vectorizer = CountVectorizer(min_df=10)\n","text_bow = vectorizer.fit_transform(preprocessed_essays)\n","print(\"Shape of matrix after one hot encodig \",text_bow.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o0l0tFAOZbmX"},"outputs":[],"source":["# you can vectorize the title also \n","# before you vectorize the title make sure you preprocess it"]},{"cell_type":"markdown","metadata":{"id":"KeRaC2KsZbmX"},"source":["## 2.2 TFIDF vectorizer"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i-Hl6qyeZbmX","executionInfo":{"status":"ok","timestamp":1666085162584,"user_tz":-330,"elapsed":12985,"user":{"displayName":"santhosh kurnapally","userId":"15491120331264307379"}},"outputId":"57d60bb2-b9e7-440c-b6ea-7c1f9ff6cbb2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of matrix after one hot encodig  (109248, 16623)\n"]}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer(min_df=10)\n","text_tfidf = vectorizer.fit_transform(preprocessed_essays)\n","print(\"Shape of matrix after one hot encodig \",text_tfidf.shape)"]},{"cell_type":"markdown","metadata":{"id":"EeMPCuY0ZbmX"},"source":["## 2.3 Using Pretrained Models: Avg W2V"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"wBejvGIBZbmX","executionInfo":{"status":"ok","timestamp":1666085234098,"user_tz":-330,"elapsed":402,"user":{"displayName":"santhosh kurnapally","userId":"15491120331264307379"}},"outputId":"92b2102f-4bb9-4efd-83e7-8adc0d0f6881"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# Reading glove vectors in python: https://stackoverflow.com/a/38230349/4084039\\ndef loadGloveModel(gloveFile):\\n    print (\"Loading Glove Model\")\\n    f = open(gloveFile,\\'r\\', encoding=\"utf8\")\\n    model = {}\\n    for line in tqdm(f):\\n        splitLine = line.split()\\n        word = splitLine[0]\\n        embedding = np.array([float(val) for val in splitLine[1:]])\\n        model[word] = embedding\\n    print (\"Done.\",len(model),\" words loaded!\")\\n    return model\\nmodel = loadGloveModel(\\'glove.42B.300d.txt\\')\\n\\n# ============================\\nOutput:\\n    \\nLoading Glove Model\\n1917495it [06:32, 4879.69it/s]\\nDone. 1917495  words loaded!\\n\\n# ============================\\n\\nwords = []\\nfor i in preproced_texts:\\n    words.extend(i.split(\\' \\'))\\n\\nfor i in preproced_titles:\\n    words.extend(i.split(\\' \\'))\\nprint(\"all the words in the coupus\", len(words))\\nwords = set(words)\\nprint(\"the unique words in the coupus\", len(words))\\n\\ninter_words = set(model.keys()).intersection(words)\\nprint(\"The number of words that are present in both glove vectors and our coupus\",       len(inter_words),\"(\",np.round(len(inter_words)/len(words)*100,3),\"%)\")\\n\\nwords_courpus = {}\\nwords_glove = set(model.keys())\\nfor i in words:\\n    if i in words_glove:\\n        words_courpus[i] = model[i]\\nprint(\"word 2 vec length\", len(words_courpus))\\n\\n\\n# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\\n\\nimport pickle\\nwith open(\\'glove_vectors\\', \\'wb\\') as f:\\n    pickle.dump(words_courpus, f)\\n\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}],"source":["'''\n","# Reading glove vectors in python: https://stackoverflow.com/a/38230349/4084039\n","def loadGloveModel(gloveFile):\n","    print (\"Loading Glove Model\")\n","    f = open(gloveFile,'r', encoding=\"utf8\")\n","    model = {}\n","    for line in tqdm(f):\n","        splitLine = line.split()\n","        word = splitLine[0]\n","        embedding = np.array([float(val) for val in splitLine[1:]])\n","        model[word] = embedding\n","    print (\"Done.\",len(model),\" words loaded!\")\n","    return model\n","model = loadGloveModel('glove.42B.300d.txt')\n","\n","# ============================\n","Output:\n","    \n","Loading Glove Model\n","1917495it [06:32, 4879.69it/s]\n","Done. 1917495  words loaded!\n","\n","# ============================\n","\n","words = []\n","for i in preproced_texts:\n","    words.extend(i.split(' '))\n","\n","for i in preproced_titles:\n","    words.extend(i.split(' '))\n","print(\"all the words in the coupus\", len(words))\n","words = set(words)\n","print(\"the unique words in the coupus\", len(words))\n","\n","inter_words = set(model.keys()).intersection(words)\n","print(\"The number of words that are present in both glove vectors and our coupus\", \\\n","      len(inter_words),\"(\",np.round(len(inter_words)/len(words)*100,3),\"%)\")\n","\n","words_courpus = {}\n","words_glove = set(model.keys())\n","for i in words:\n","    if i in words_glove:\n","        words_courpus[i] = model[i]\n","print(\"word 2 vec length\", len(words_courpus))\n","\n","\n","# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n","\n","import pickle\n","with open('glove_vectors', 'wb') as f:\n","    pickle.dump(words_courpus, f)\n","\n","\n","'''"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"a2J7aLpxZbmY","executionInfo":{"status":"ok","timestamp":1666085416688,"user_tz":-330,"elapsed":2480,"user":{"displayName":"santhosh kurnapally","userId":"15491120331264307379"}}},"outputs":[],"source":["# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n","# make sure you have the glove_vectors file\n","with open('/content/drive/MyDrive/6_Donors_choose_NB/glove_vectors', 'rb') as f:\n","    model = pickle.load(f)\n","    glove_words =  set(model.keys())"]},{"cell_type":"code","execution_count":13,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"CCncTursZbmY","executionInfo":{"status":"ok","timestamp":1666085533210,"user_tz":-330,"elapsed":43085,"user":{"displayName":"santhosh kurnapally","userId":"15491120331264307379"}},"outputId":"9b4b3250-5ecd-4696-a43f-39743a0997f6"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 109248/109248 [00:42<00:00, 2545.11it/s]"]},{"output_type":"stream","name":"stdout","text":["109248\n","300\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# average Word2Vec\n","# compute average word2vec for each review.\n","avg_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n","for sentence in tqdm(preprocessed_essays): # for each review/sentence\n","    vector = np.zeros(300) # as word vectors are of zero length\n","    cnt_words =0; # num of words with a valid vector in the sentence/review\n","    for word in sentence.split(): # for each word in a review/sentence\n","        if word in glove_words:\n","            vector += model[word]\n","            cnt_words += 1\n","    if cnt_words != 0:\n","        vector /= cnt_words\n","    avg_w2v_vectors.append(vector)\n","\n","print(len(avg_w2v_vectors))\n","print(len(avg_w2v_vectors[0]))"]},{"cell_type":"markdown","metadata":{"id":"0FA0PgUlZbmY"},"source":["## 2.4 Using Pretrained Models: TFIDF weighted W2V"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"O57qH4wiZbmY","executionInfo":{"status":"ok","timestamp":1666085565645,"user_tz":-330,"elapsed":13152,"user":{"displayName":"santhosh kurnapally","userId":"15491120331264307379"}}},"outputs":[],"source":["# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\n","tfidf_model = TfidfVectorizer()\n","tfidf_model.fit(preprocessed_essays)\n","# we are converting a dictionary with word as a key, and the idf as a value\n","dictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\n","tfidf_words = set(tfidf_model.get_feature_names())"]},{"cell_type":"code","execution_count":15,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"7of_nnJWZbmY","executionInfo":{"status":"ok","timestamp":1666085930260,"user_tz":-330,"elapsed":258399,"user":{"displayName":"santhosh kurnapally","userId":"15491120331264307379"}},"outputId":"bd1d42be-ebcd-4194-b055-2d8867f86993"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 109248/109248 [04:17<00:00, 424.86it/s]"]},{"output_type":"stream","name":"stdout","text":["109248\n","300\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# average Word2Vec\n","# compute average word2vec for each review.\n","tfidf_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n","for sentence in tqdm(preprocessed_essays): # for each review/sentence\n","    vector = np.zeros(300) # as word vectors are of zero length\n","    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n","    for word in sentence.split(): # for each word in a review/sentence\n","        if (word in glove_words) and (word in tfidf_words):\n","            vec = model[word] # getting the vector for each word\n","            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n","            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n","            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n","            tf_idf_weight += tf_idf\n","    if tf_idf_weight != 0:\n","        vector /= tf_idf_weight\n","    tfidf_w2v_vectors.append(vector)\n","\n","print(len(tfidf_w2v_vectors))\n","print(len(tfidf_w2v_vectors[0]))"]},{"cell_type":"markdown","metadata":{"id":"-AJUHDZ2ZbmY"},"source":["# 2. Vectorizing Categorical Features"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZDlBWLWDZbmY","executionInfo":{"status":"ok","timestamp":1666085964786,"user_tz":-330,"elapsed":500,"user":{"displayName":"santhosh kurnapally","userId":"15491120331264307379"}},"outputId":"e3d7c94f-8c0f-414e-eb63-5a1d14dabd6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of matrix after one hot encodig  (109248, 51)\n"]}],"source":["# provided we did the cleaning\n","vectorizer = CountVectorizer(binary=True)\n","school_state_ohe = vectorizer.fit_transform(data['school_state'].values)\n","print(\"Shape of matrix after one hot encodig \",school_state_ohe.shape)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}